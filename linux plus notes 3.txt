Linux storage system overview
-----------------------------

Types of Storage
 - File storage
 - Block storage
 - Object storage

File Storage
 - Hierarchy of files in folders
 - Data in files
 - Metadata contains path to file
 - Local file systems
 - Network file systems - NFS (Network File System), CIF (Windows Common Internet Filesystem)

Block Storage
 - Exposes blocks of binary data
 - Blocks have unique identifiers
 - Storage system reassembles blocks
 - Storage area networks (iSCSI)

Object Storage
 - Manages data
 - Links data to associated metadata
 - Data is stored in discrete objects
 - Single repository
 - Red Hat Ceph

ATA Drives
 - local drives in a legacy system
 - Parallel ATA drive with a flexible ribbon cable with 80 wires
 - Two drives per cable
	To have more than two drives, a system needs multiple controllers
 - Speed: 133 MB/sec
 - /dev/hda, hdb

SCSI Drives
 - Small Computer System Interface
 - higher end legacy systems
 - Parallel SCSI drive with larger flat ribbon cable with 168 wires, used mostly for servers
 - 16 drives per cable
 - Speed: 320 MB/sec
 - /dev/sda, sdb

SATA Drives
 - newer systems
 - Serial ATA with narrow serial cable
 - One drive per cable
	Can be a spinning disk, or a Solid State Drive
 - we measure serial interfaces with Bits per second instead of Bytes
 - Speed: 6 Gbps
 - /dev/sda, sdb

SAS Drives
 - Serial Attached SCSI
 - Parallel SCSI was replaced by Serial Attached SCSI (SAS)
 - Backward compatible with SATA
 - Speed: 12 Gbps
 - /dev/sda, sdb

NVMe Drives
 - Non-Volatile Memory Express
 - no cable at all, plugs into slot like a memory stick
 - The structure of the internal components is very similar to a solid state SATA drive,
	the only difference being the interfaces direct via a PCI Express bus
 - Speed: 32 Gbps
 - /dev/nvme0n1p1
	nvme followed by device number, namespace number, and partition number

USB Drives
 - Thumb drive
 - SSD in USB case
 - Speed: 5-20 Gbps
 - /dev/sda, sdb

Partitioning the drive = divide drive into smaller, more manageable sections
OS	Users	Data

 - Older BIOS space systems can only have 4 primary partitions with a maximum size of 2 TB
	A later trick was developed to allow one of the four partitions to be made into an extended partition,
	 and then divided again into a maximum of 12 logical drives.
 - The BIOS partition table is stores in the master boot record in the first sector of the drive
 - BIOS was replaced by the Unified Extensible Firmware Interface (UEFI)
 - One of the improvements was the GUID Partition Table (GPT) which supports up to 128 partitions
	with a total capacity of 9.4 Zetabytes (ZB)(9.4 billion Terabytes)

 - Linux has the additional ability to combine partitions and drives into volume groups,
	and divide the resulting group into smaller logical volumes
 - This is called LVM

[Part 1]		[Part 2]		[Part 3]
↓			↓			↓
			[Volume Group]
↓			↓			↓
[Logical Volume]	[Logical Volume]	[Logical Volume]

 - New drives can be added to the volume group allowing you to increase the size of the logical volume without
	taking it offline.
 - Each logical volume acts as a flexible partitions.
 - It also allows you to swap drives from underneath the logical volumes while they're being used.
 - Unlike partitions, space utilized for logical volumes does not need to be continuous.

 - Drives can also be combined into a RAID (Redundant array of independent disks)

[Drive 1]	[Drive 2]
↓		↓
	[/dev/sda]

 - RAID can be accomplished on Linux using LVM or the mdadmin tool

 - Drives, partitions, logical volumes, and RAID devices can be combined in many different ways to solve your
	storage needs.

[Drive 1]	[Drive 2]	[Drive 3]	[Drive 1]	[Drive 2]	[Drive 3]
↓		↓		↓			↓		↓		↓
		[/dev/sda]					[/dev/sdb]
		↓							↓
				[Volume Group]
				↓
				[Logical Volume]
				↓
				[EXT4]

 - The resulting device could be encrypted using LUKS for security [LUKS Encrypted Volume] (instead of EXT4),
	or just formatted with a file system mounted on a directory to allow access
	 [ext4 - /media/storage]

Linux File Systems (current)
 - ext4
 - XFS
 - Btrfs

Legacy Linux File Systems
 - ext2
 - ext3
	Added journal
 - Maximum file size: 2 TB
 - Maximum file system size: 16 TB

ext4 File System
 - Supports SSDs
 - Maximum file size: 16 TiB
 - Maximum file system size: 1 EiB (1 billion Gigabytes)
 - ext2/ext3 can migrate to ext4 without reformatting

FXS File System
 - Default on Enterprise Linux from version 7
 - 64-bit file system
 - Created by Silicon Graphics for Unix
 - Maximum file system size: 8 EiB

Btrfs File System (pronounced butter FS)
 - Created as a response to Sun Microsystems ZFS
 - Supports:
	- Partitioning
	- RAID
	- Logical volumes
	- Snapshots
	- Compression
 - Maximum file size: 16 EiB

 - When it comes to virtual memory, instead of using a page file stored on disk, Linux by default uses an
	entire partition
 - This provides a faster virtual memory interface that is not subject to fragmentation,
	but does require a separate partition or logical volume.

Specialty File Systems
 - Enterprise Cryptographic File System (eCryptfs)
	Applies POSIX-compatible encryption protocols to data before it's written to disk

Non-Linux File Systems (supported by Linux)
 - VFAT/exFAT (Microsoft)
 - NTFS (Microsoft)
 - HFS/HFS+ (Apple)
 - ISO 9660/UDF (optical disks)
 - ZFS (Sun Microsystems)

 - There are licensing issues with some of these file systems, restricting them from being included in the kernel.
 - They're handled in user space directly or through the FUSE file system, which acts as a bridge between the 
	kernel and user-created file systems

Kernel Space		User Space
[Kernel]	↔	[FUSE]↔[ZFS]

Remote File Systems (supported by Linux)
 - SMB (Windows)
 - CIFS (Windows)
 - NFS (UNIX)

==========

Prepare for handling storage
----------------------------

To practice in VirtualBox

 - Shut down VM
 - Settings
 - Storage
 - Right click Controller: SATA
 - Hard Disk
 - Create
 - Keep default of VDI
 - For storage type keep default of dynamically allocated so it doesn't take up extra disk space
 - For LVM and RAID exercises, you need a lot of drives, but they don't need to be very large so change the size
	to 1 GB
 - Also take note of the virtual disk name, i.e. rhhost1_1.vdi
 - Click Finish
 - Click Choose to attach to VM
 - Create 3 more drives the same way, incrementing the name for each one
 - Power VM back up again

==========

Create partitions using fdisk
-----------------------------

cat /proc/partitions
	get a list of drives and partitions that the kernel recognizes, comes from the kernel directly
lsblk
	list block devices
	gives a visual list of the drives and their partitions or logical volumes
sudo blkid
	list partition labels and UUID numbers
	it also shows which file system the drive is formatted with
sudo fdisk -l
	read the partition table from the drive directly
sudo fdisk -l /dev/sda
	list partitions on only one drive

 - fdisk is getting the data directly from the drive's partition table
 - /proc/partitions lists the partitions that the kernel recognizes, and is stored in RAM
 - In some cases, if you modify a drive's partition, the kernel may not update it's partition table because the
	drive is locked by a process, or because one of its partitions is mounted.
	If possible, unmount the partition and then use partprobe to update the partition table.

sudo partprobe
	update the partition table
	then check /proc/partition again
	if this still doesn't match fdisk, you may have to reboot

 - Legacy systems use a BIOS to boot the machine (rather than UEFI and GUID partition table)
 - BIOS stores its partition table in the MBR - master boot record
 - Although fdisk supports GUID partitions now, older versions did not, so to edit partitions you had to use gdisk
 - Everything we do here in fdisk can be done in gdisk as the interface is virtually the same

sudo fdisk /dev/sdb
	edit the partitions on /dev/sdb
	if no partition table is found, a new DOS partition table is created (old BIOS style partition table)
m
	help menu
d
	delete a partition
n
	create a new partition
p
	print partition table
t
	change a partition type
w
	write partition table to disk and exit
q
	quit without saving changes
g
	create a new GPT partition table
	this is desirable unless you're on a legacy system with BIOS

g
n
1
2048
+500M
	create a 500MB partition and leave the rest of the disk free
p
	print partition table to verify
w
	write and exit

sudo udevadm settle
	register the partition to let the kernel know it has changed
cat /prov/partitions
	verify the kernel recognizes the new partition

 - Once you've created partitions, you can use them in LVM or format them directly using tools such as mkfs
 - Once you have formatted it, you can mount it

==========

Create partitions using parted
------------------------------

Older versions of parted could:
 - resize partitions
 - copy partitions
 - move partitions
 - format partitions
 - rescue

Some of these features were removed after version 2.4 including:
 - check
 - cp
 - mkpartfs
 - move
 - resize

 - gparted is a GUI tool, but is different from parted

sudo parted
	run parted in interactive mode
help
	show list of commands
help print
	show more info about a specific command such as print
print
	display the partition table, much like typing p in fdisk
print all
	print all drives and partitions, similar to fdisk -l
print devices
	a shorter more concise list drives
select /dev/sdc
	select a specific drive to edit
print free
	get stats on the drive and show free space
mktable gpt
	this is the same as 'mklabel'
	created a GPT style partition table
print
	verify partition table was created
mkpart primary 1MiB 500MiB
	create a new 500 MB partition now they we have a partition table
	the choice of type can be primary, extended, or logical
	because we created a GPT partition table, all partitions will be primary
quit
	exit parted
sudo udevadm settle
	register the partition to let the kernel know that it has changed
	you may not have to do this depending on which drive you're partitioning but it's a good practice
cat /proc/partitions
	verify that the kernel recognizes the new partition

 - creating a bare/raw drive in Virtual Box will likely not have a partition table at all
 - when you buy physical drives, usually they come with a partition table already
 - once you've created partitions, you can use them in LVM or format them directly using disk formatting tools
	such as mkfs

==========

Manage LVM volumes and volume groups
------------------------------------

Logical Volume Management (LVM)
 - resizing volumes
 - combining volumes
 - moving volumes
 - adding additional drives
 - hot replacing underlying drives

lsblk
	verify partition exists
sudo pvcreate /dev/sdb1
	prepare partition for LVM
sudo pvs
	physical volume summary
	verify the volume format is now LVM
sudo pvdisplay
	show more info about physical volumes
sudo vgcreate vgdata /dev/sdb1
	create a volume group named vgdata and include our physical volume
sudo vgs
	verify with volume group summary
sudo vgdisplay
	show more volume group information

 - To reference volume groups use the name only without the path
 - Volume groups are an abstraction, so they don't have a physical location

sudo lvcreate --name lvdata --size 495M vgdata
	create a logical volume inside of the volume group
	This will create a new logical volume that's 495 MB in size, named lvdata in the vgdata volume group.
	We have to make the logical volume slightly smaller than the volume group to be sure that it will fit.
sudo lvs
	verify
sudo lvdisplay
	more info

 - There are two different paths you can use to refer to logical volumes

LVM Volume Path Names
 - /dev/VolumeGroupName/LogicalVolumeName
 - /dev/mapper/VolumeGroupName-LogicalVolumeName

Formatting tools
 - mkfs
	simpler
 - mke2fs
	more powerful

sudo mkfs -t ext4 /dev/vgdata/lvdata
	format our logical volume as ext4
sudo blkid
	verify

 - To make the logical volume accessible, you'll need to mount it

sudo mkdir /media/lvdata
	create a mount point
sudo mount /dev/vgdata/lvdata /media/lvdata
	mount the logical volume
df -Th
	show file system including type and human readible format

 - Drives and logical volumes mounted manually won't survive a reboot without adding them to the /etc/fstab
	the file system table

sudo vi /etc/fstab
	edit
i
	insert mode

add the end of the file add:
/dev/vgdata/lvdata /media/lvdata ext4 defaults 1 2

/dev/vgdata/lvdata	/media/lvdata	ext4		defaults		1	2
logical volume path	mount point	file system	filesystem options

defaults include rw, suid, dev, exec, auto, nouser, async
1 = the fifth volumn is to include this drive in a backup using the dump command
2 = the last column is the order to do a file system check on bootup
	root should always be 1, and other drives 2 or higher if to be checked, and 0 if not

esc, :x!
	save and exit

sudo umount /media/lvdata
	unmount volume
sudo mount -a
	reads fstab and attempts to mount all drives that are not mounted

==========

Expand existing logical volumes
-------------------------------

cat /proc/partitions
	list partitions
	if a partition didn't exit, it would need to be created using fdisk or parted

sudo pvcreate /dev/sdc1
	make sdc1 a physical volume
sudo pvs
	verify

 - Normally we would create a new volume group at this point, but for this exercise we're going to extend an
	existing volume group

sudo vgs
	list all volume groups
sudo vgextend vgdata /dev/sdc1
	extend the vgdata volume group by adding the new physical volume to it

 - The next step is to resize the logical volume

sudo lvs
	verify logical volume size
sudo lvresize -l 100%VG /dev/vgdata/lvdata
	resizing logical volume
	when we're specifying the logical volume, we provide the full path to it
df -h
	verify the final system size of lvdata

 - Since/if lvdata is still mounted, the file system size is the old size
 - Because we formatted our logical volume as ext4, we'll resize it using resize2fs
 - lvresize command can now resize the filesystem as well, older versions could not

sudo resize2fs /dev/vgdata/lvdata

df -h
	verify again, logical volume and it's file system are expanded to the new size

==========

Reduce existing logical volumes
-------------------------------

 - lvresize can be used to unmount and remount volumes, as well as resize file systems.
 - We're going through the manual process first, as older versions may not support these features.
 - Although it's a good idea to always unmount a file system when resizing it, we can leave it mounted when
	increasing the size.
 - However, when we decrease the size, we have no choice but to unmount the volume first.
 - Some filesystems like XFS can not be resized down, only up.

sudo umount /dev/vgdata/lvdata
	unmount volume
sudo e2fsck -ff /dev/vgdata/lvdata
	run a file system check
	if the filesystem passes all 5 checks, then proceed
sudo resize2fs /dev/vgdata/lvdata 500M
	resize filesystem
sudo lvresize -L 500M /dev/vgdata/lvdata
	reduce the logical volume size to 500 MB
sudo lvs
	verify
sudo mount /dev/vgdata/lvdata /media/lvdata
	mount filesystem
df -h
	check size

sudo lvresize -r -L 400M /dev/vgdata/lvdata
	will prompt to unmount
	will reduce the logical volume using lvresize, newer method
sudo lvresize -r -L 600M /dev/vgdata/lvdata
	this will increase or decrease size based on what's specified vs the size it already is

 - Using the newer syntax, lvresize is doing all of the legwork for us by unmounting it, resizinf the filesystem,
	reizing the logical volume, and mounting it back up.

==========

Create EXT filesystems
----------------------

 - Before you can use a partition or logical volume in Linux, you need to put a file system on it.
 - This process is called formatting the drive.
 - The standard tool to format filesystems is mkfs

ls /sbin/mk*
	show all mkfs programs

Reformat lvdata logical volume:

sudo umount /dev/vgdata/lvdata
	unmount if it's still mounted
df
	verify the volume is not mounted
sudo mkfs /dev/vgdata/lvdata
	format with default filesystem (by not specifying one)
	answer y when prompted
sudo lsblk -f
	verify with lsblk which will show us the filsystem used

 - The only difference between ext2 and ext3 is the journal.
 - ext4 is recommended.
 - tune2fs can convert our filesystem from ext2 to ext3
	tune2fs also allows you to modify attributes of ext filesystems including
		- name
		- ID
		- how many times a filesystem can be mounted before checked
		- filesystem optimization settings

sudo tune2fs -j /dev/vgdata/lvdata
	-j creates the journal
sudo lsblk -f
	now we can see it's formatted as ext3
	you can go back and forth between these two without losing data

 - In recent versions of Linux, the ext4 driver is used to format all ext file systems.
 - Older systems had separate drivers for ext2 and 3.
 - You can migrate ext2 and ext3 to ext4 without losing data, in a one-way process.

sudo tune2fs -O extent,uninit_bg,dir_index /dev/vgdata/lvdata

extent = specifies using extent trees to store the location of data blocks
	this is an ext4 feature
uninit_bg = speeds up subsequent file system checks after the first one is completed
dir_index = uses hashed b-trees to speed up lookups for large directories

sudo e2fsck -fD /dev/vgdata/lvdata
	check the file system (5 passes)
sudo mount /dev/vgdata/lvdata /media/lvdata
	it's ready to be mounted
df -Th
	verify it's mounted, and file system type

==========

Repair EXT filesystems
----------------------

Corrupting the filesystem! 😈

[ Do not attempt on a production machine; test only ]

 - In order to fix the file system we'll need to break it first.
 - Make sure /dev/vgdata/lvdata is mounted as /media/lvdata.
	If not, then mount it:
		sudo mount /dev/vgdata/lvdata /media/lvdata/

sudo cp -Rvf / /media/lvdata
	Recursively copy a bunch of files to the volume.
	Let it run for a while and then kill the process with ctrl+c.
		You need a lot of files in the file system so you have a higher chance of corrupting one of them.
sudo dd if=/dev/zero bs=1 count=10 of=/dev/vgdata/lvdata seek=1000
	Using dd to copy zeros to random places on the file system.
	bs=1 -> block size is 1
	count=10 -> that'll be 10 blocks
	We're writing to the logical volume device, not to the file system.
	Run this multiple times and change the seek value each time.

sudo umount /media/lvdata
	unmount
sudo fsck -n /dev/vgdata/lvdata
	Run a file system check.
	Using -n will notify of corruption but not fix it.

Option		Function
fsck -A		Checks all file systems
fsck -AR	Checks all file systems except for the root
fsck -f		Force checks all file systems, even if they are considered clean
fsck -a		Fixes safe problems automatically.
		These are issues that don't need review from an administrator.
fsck -y		Answers all questions with a yes.
		This is not the same thing as -a
		There are some decisions that an administrator needs to make.
fsck -n		Doesn't fix anything, displays results

sudo fsck /dev/vgdata/lvdata
	it will prompt you through each fix

 - Since it's a bad idea to check filesystems that are mounted, it's difficult to check filesystems that are
	always mounted such as root.
 - However, you can force a file system to check on the next reboot.
 - In a legacy Linux system created before systemd, you can create a file in / named forcefsck and then reboot.

sudo tune2fs -l /dev/vgdata/lvdata
	Check for 'Maximum mount count'.  -1 means that the max. times it can be mounted before forcing a file
	 system check has been turned off.
	'Mount count' shows how many times it has been mounted.
	'Last checked' shows the last time the file system was checked.
sudo tune2fs -c 1 /dev/vgdata/lvdata
	Enable the check.
	Then reboot.
sudo tune2fs -c -1 /dev/vgdata/lvdata
	Disable check again.

==========

Create and repair XFS filesystems
---------------------------------

 - Unmount volume if it's mounted.

sudo mkfs -t xfs -f /dev/vgdata/lvdata
	Reformat logical volume as XFS.
	-f for force.
lsblk -f
	Verify.
sudo xfs_repair /dev/vgdata/lvdata
	Repair XFS filesystem.

XFS Tools
Command		Function
xfs_admin	Changes file system parameters.
		i.e. how it works, and the filesystem label.
xfsdump		File system backup tool.
		Incremental filesystem dump ability.
xfs_freeze	Suspends access to a file system.
xfs_quota	Handles XFS file system quotas.
xfs_growfs	Resizes XFS file systems larger.

XFS does not allow shrinking, only grow.

==========

MDRAID or DMRAID?
-----------------

MDRAID
 - Replaced RAID tools
 - Administer with mdadm
 - Software RAID works at the device level.
 - /dev/mdx device names
 - To create a RAID, we grab multiple devices such as /dev/sdb and /dev/sdc and combine them into a RAID.
 - The Linux MD stack would then give us a new device name such as /dev/md0, which we'd format and mount.

MDRAID RAID Levels
 - RAID 0
 - RAID 1
 - RAID 4
 - RAID 5
 - RAID 6
 - RAID 10
 - Linear
 - Multipath

 - LVM has been slowly adding RAID capabilities to a stack.  This is referred to as DMRAID, or device mapper RAID.

DMRAID
 - Uses device mapper
 - Administer with LVM
 - /dev/volumegroup/logicalvolume device names

DMRAID RAID Levels
 - Linear volumes
 - RAID 0
 - RAID 1
 - RAID 4
 - RAID 5
 - RAID 6
 - RAID 10
 - Mirrors
	Up to four mirrors of one volume.

 - The future may belong to LVM as developers add features.
 - In some cases, LVM actually uses the MDRAID stack underneath when it makes sense.
 - mdadm is still the recommended way of creating software RAIDs in Enterprise Linux 7, while LVM is the recommended
	way of creating non-RAID volumes.

==========

Create a RAID 5 using LVM
-------------------------

RAID Levels
Level	Function
0	Striping; no redundancy; requires two drives minimum (it's fast)
	The more drives, the faster the throughput--up to a point.
1	Mirroring; drives mirrored in pairs
	The contents of the first drive is the same as the second.
	(slower than single drives because data is written twice)
10	Disk mirroring and striping; requires four drives
	Essentially two mirrors combined in a stripe to regain some of the speed lost from the mirror.
4	Striping with parity; requires three drives minimum
	A stripe is created across all but one drive, which stores a parity bit so if a drive fails it can be
	 recreated.
5	Striping with striped parity; requires three drives minimum
	Similar to RAID level 4 but the parity data is striped across the array increasing speed
6	Striping with two levels of redundancy; requires four drives minimum
	2 drives can fail and still be able to recover

 - We are going to create a RAID 5
 - Make sure that /dev/vgdata/lvdata is not mounted

sudo vgremove vgdata
	remove the group
	it will ask if you want to remove the volume group vgdata,
	 and remove active logival volume vgdata/lvdata
	 You can pick y for both.

sudo lvs
	verify
sudo vgs
	verify

 - You'll also want to look at the /etc/fstab to remove any rentries referencing this logical volume and either
	delete them or comment them out.
 - If you do not do this and reboot, you will end up in emergency mode and you'll have to fix it there.

sudo vi /etc/fstab
	i for insert mode
	add # at beginning of line to comment out
	exit with esc :x! enter

 - Now let's create a RAID array with 3 drives
 - First, use fdisk to create a new partition

sudo fdisk /dev/sdd
	open fdisk interactive mode
g
	change drive to GPT label
n
	create a new partition
	take the default of 1 and hit enter
	take the default first sector of 2048 and hit enter
	for the ending sector, we're going to specify a size of +500M to match the partitions of the other drives
p
	print partition table to verify
w
	write partition table and exit

 - Let's create a volume group and include all three partitions.
 - We have not made sdd1 a physical volume. However, an interesting shortcut is letting vgcreate do it automatically

sudo vgcreate vgraid /dev/sddb1 /dev/sdc1 /dev/sdd1
	If it prompts you to wipe the file system say yes
pvs
	verify

 - At this point, there's nothing special about our volume group as it's no different than the non-RAID volume
	group we made previously.
 - Now that we have the volume group and it includes three partitions, let's create the logical volume.
 - Here's where we specify the RAID.

sudo lvcreate --type raid5 -i 2 -l 100%VG -n lvraid vgraid
	We have three drives, but the width of a RAID5 stripe is the total number of drives minus one,
	 so in our case, 3 - 1 is 2.
	We specify 100%VG for the size so it will take up as much space as you have in the volume group.
sudo lvs
	-n = name, lvraid; lastly the name of the volume group (vgraid)
	verify
	here we can see that it's 100% synchronized

 - Notice that the capacity of our RAID is not equal to the capacity of the volume group.
 - The capacity formula for RAID5 is capacity = nDrives -1 x (capacity of smallest drive)
 - In our case it's the smallest drive x 2, or about 984 MB
 - Now let's format our RAID with the ext4 file system.

sudo mkfs -t ext4 /dev/vgraid/lvraid

sudo mkdir /media/lvraid
	make a mount point for it
sudo mount /dev/vgraid/lvraid /media/lvraid
	mount it
df -h
	verify

Tearing down an LVM RAID in order (the same as any other volume):
 - umount
 - lvremove
 - vgremove
 - pvremove

 - You can use the fio command to do disc speed tests on different configurations.
 - Look into RAID-specific file system optimizations.

==========

Create RAIDs using mdadm
------------------------

 - Clear out old volumes: start by deleting the volume group and logical volume that you created previously
 - Make sure the logical volume is not mounted first and then remove it

sudo umount /media/lvraid

sudo vgremove vgraid

sudo pvremove /dev/sdb1
sudo pvremove /dev/sdc1
sudo pvremove /dev/sdd1

 - When using mdadm you can create RAID from partitions, or you can create RAID from the drives themselves
 - An advantage to having partitions for your RAID is that you can set the partition type to RAID which allows
	your system to recognize the drives ar boot for assembling the RAID
 - Let's use fdisk to go into each partition and change the type to Linux RAID

sudo fdisk /dev/sdb
	press t for type
	press L to view the partition types
	press q, then type 29 and hit enter
	press p to print partition table
	press w to write and exit

 - Do this for each of the drives that have partitions

sudo fdisk /dev/sdc
	t
	29
	w
sudo fdisk /dev/sdd
	t
	29
	w

 - Now let's add a partition to /dev/sde so we can create a four drive RAID 5

sudo fdisk /dev/sde
	g = change to GPT label
	n = create new partition
	1 = take default partiton number
	2048 = take default first sector of 2048
	+500M = specify 500M to match the partitions on the other drives
	t = change type
	29 = Linux RAID
	p = print
	w = write and exit

lsblk
	verify

sdb
--sdb1
sdc
--sdc1
sdd
--sdd1
sde
--sde1

mdadm Operating Modes

Mode			Function
Assemble		Assembles preexisting array
Incremental assembly	Adds single device to array
Create			Creates a new array and writes metadata to disks
			Activates the array and it can be used immediately, even if it's in the process of
			 resyncing drives.
Build			Builds an array from drives without RAID metadata
			It's a specific use-case and should only be done if you know what you're doing
Monitor			Monitors RAID devices and acts on state change (i.e. failed drive)
Grow			Grows, shrinks, or changes an array
			Even used if you want to change RAID levels
Manage			Adding new spare drives or replacing failed drives
Misc			Miscellaneous items such as removing metadata from drives

 - Let's use the Create mode to create a brand new RAID 5 array using four drives

sudo mdadm --create /dev/md/mdraid /dev/sdb1 /dev/sdc1 /dev/sdd1 /dev/sde1 --level=5 --raid-devices=4
 --bitmap=internal
	/dev/md/mdraid = device name

lsblk
	verify what we've created

sdb
--sdb1
  --md127
sdc
--sdc1
  --md127
sdd
--sdd1
  --md127
sde
--sde1
  --md127

 - mdadm starts at md127 and counts backwards for the device number

ls -l /dev/md/mdraid
	lxrwxrwxrwx. 1 root root 8 Dec 29 13:19 /dev/md/mdraid -> ../md127
	we can see that it's a link that points to /dev/md127

 - We can format either the RAID number (md127) or the path to the name.
 - The name is more reliable as the number may change if you have more than one array

cat /proc/mdstat
	get status
		md127 : active raid5 sde1[4] sdd1[2] sdc1[1] sdb1[0]
		1526784 blocks super 1.2 level 5, 512k chunk, algorithm 2 [4/4] [UUUU]

sudo mdadm --detail /dev/md/mdraid
	get more deatils with mdadm
	this shows the device name and the drives that are in it

		Number	Major	Minor	RaidDevice	State
		0	8	17	0		active sync	/dev/sdb1
		1	8	33	1		active sync	/dev/sdc1
		2	8	49	2		active sync	/dev/sdd1
		4	8	65	3		active sync	/dev/sde1


sudo systemctl enable mdmonitor
sudo systemctl start mdmonitor
	enable and start the status to have mdmonitor keep an eye on your RAID

 - Now let's format the drive with the XFS file system

sudo mkfs -t xfs /dev/md/mdraid

sudo mkdir /media/mdraid
	create a mount point
sudo mount /dev/md/mdraid /media/mdraid
	mount
df
	verify
lsblk
	verify

Other useful mdadm options:

sudo mdadm --query /dev/md/mdraid
	summary of array information
sudo mdadm --detail /dev/md/mdraid
	detailed info
sudo mdadm --fail /dev/sdb1
	mark drive as failed, sets a device as faulty
	this is the first step in replacing a drive
sudo mdadm --remove /dev/sdb1
	remove drive
sudo mdadm --add /dev/sdb1
	add new drive
sudo mdadm --stop /dev/md/mdraid
	stop a drive array
	then remove
	this is the same option as you'd use to remove a device but if you specify an array, it removes the entire
	 array

 - If you decide to remove the array, you'll want to also delete any RAID metadata on the drives to keep the RAID
	system from trying to reassemble the array.  Use --zero-superblock

sudo mdadm --zero-superblock /dev/sdb1
	remove RAID superblock from drive

 - mdadm RAIDs can be a bit sticky and hard to get rid of.  Let's break it down

sudo umount /media/mdraid
sudo mdadm --stop /dev/md/mdraid
sudo mdadm --zero-superblock /dev/sdb1
sudo mdadm --zero-superblock /dev/sdc1
sudo mdadm --zero-superblock /dev/sdd1
sudo mdadm --zero-superblock /dev/sde1
sudo systemctl stop mdmonitor
sudo systemctl disable mdmonitor

 - You will also want to change the partition type of each partition back to non-RAID Linux partitions

sudo fdisk /dev/sdb
	t
	linux
	w
sudo fdisk /dev/sdc
	t
	linux
	w
sudo fdisk /dev/sdd
	t
	linux
	w
sudo fdisk /dev/sde
	t
	linux
	w

==========

Mount filesystems on boot
-------------------------

 - If you unplug a  drive, the order of drives might break a system if they are in the fstab with static pass.
 - It's better to mount drives based on a label or UUID.

 - First we will create two more partitions on the /dev/sde drive.
 - First one is 250 MB:
sudo fdisk /dev/sde
	n
	enter
	enter
	+250M
 - Next one will take up the rest of the drive:
	n
	enter
	enter
	enter

 - There should now be three partitions:
	p

 - Write changes:
	w

 - Register the new partitions
sudo udevadm settle

 - Verify that the kernel sees the partitions:
cat /proc/partitions

 - Format both partitions as ext4:
sudo mkfs -t ext4 /dev/sde2
sudo mkfs -t ext4 /dev/sde3

 - Verify that they are both formatted:
sudo blkid

 - Create mount points
sudo mkdir /media/sde2
sudo mkdir /media/sde3

 - Create an entry in fstab
sudo vi /etc/fstab
	- append to the end:
	UUID=bb67878-dis237d-sdgy8-sd8ygf	/media/sde2	ext4		defaults	0	0
	device name				mount point	filesys type	options		backup	filesyscheck

 - For the file system check column, root should always be 1, and other drives 2 or higher if to be checked, or 0 if not.

 - For the second partition, we'll mount it by label.
 - File system labels are specific to the file system.
	ext4 formatted partitions will use e2label command
	xfs would use fxs_admin command

sudo e2label /dev/sde3 backups
	backups is the label
sudo e2label /dev/sd3
	verify
sudo tune2fs -l /dev/sde3
	get more information

sudo vi /etc/fstab
	Append at the end:
	LABEL=backups	/media/sde3	ext4	defaults	0	0

sudo mount -a
df -h
	Verify that the lines in /etc/fstab will work (make sure they aren't mounted first)

 - If you delete a partition or logical volume, make sure that you remove it from the fstab immediately where it may impact your ability to boot your Linux system.

==========

Encrypt drives with LUKS
------------------------

Linux Unified Key Setup = LUKS

cat /etc/fstab
	If there is any line in this file referencing /dev/sde1 then delete it.
	If you don't and the system reboots before you're done making changes, it can interrupt the boot process
	 and you'll need to fix the problem in emergency mode.

sudo cryptsetup -y -v luksFormat /dev/sde1
	YES
		agree to overwrite data on /dev/sde1
		enter new password
sudo cryptsetup -v luksOpen /dev/sde1 decryptvolume
	Make the decrypted device available for use.
	The last argument is the name that we'll use for the device, you can change this to whatever.
	This will prompt for password.
ls -l /dev/mapper
	Look at device maps.
sudo mkfs -t ext4 /dev/mapper/decryptvolume
	Give it a file system.
	Now you can mount it, copy files to it, etc. just liek if it were a normal formatted partition.
sudo cryptsetup -v luksClose /dev/mapper/decryptvolume
	Deactivate the dexrypted volume.
	Now it's no longer accessible and all of the data in the drive is safely secure.

==========

Troubleshoot storage systems
----------------------------

 - Troubleshooting storage issues starts with making sure you are not out of space.

df -hT
	Shows the overall storage space usage of a mounted volume.
	h = human readable sizes.
	T = file system type.
du
	Tell you the size of a directory.
	Requires an argument.
	For example: du -h ~

 - If you're experiencing slow disk performance, it may be due to the drive not being able to handle the number of I/O operations that have been requested.  You can use the iostat command to analyze this.  If it's not installed, install the sysstat package to get it.

sudo dnf install -y sysstat

sudo iostat -d
	Show drive statistics.
	adding -x shows even more info.
sudo iostat -xdt 2 5
	Display statics 5 times with 2 seconds between them.

 - To find applications using a lot of disc I/O, use the iotop program.

sudo dnf install -y iotop

sudo iotop
	Run iotop application.
	Similar to top, but for I/O.

 - Once you've identified the applications using excessive I/Os, you can then make a decision what to do.
 - One option if youy have high disk I/O is to change to a different scheduler.
 - Linux allows you to swap out the I/O schedulers to tune your system.

Linux I/O Schedulers:
 - CFQ scheduler: places I/Os in queues for each application and then rotates between the queues.
	It places priority on read I/Os.
	It's a nice balanced scheduler.
 - Deadline scheduler: batch processes I/O requests by time.
	Is generally preferred for database operations when latency is important.
 - NOOP scheduler: places all I/O requests in one FIFO (first in, first out) and processes them in order.
	Good if lower CPU utilization is important.

cat sys/block/<drive name>/queue/scheduler
	View current I/O scheduler.
echo noop > /sys/block/<drive name>/queue/scheduler
	Set I/O scheduler.
	You'll need to change to the root user to do this; Using the su command as redirects don't work well
	 with sudo.

 - To measure disk performance, you can use the fio command that's in the fio package (it will need to be installed).

fio --name=random-write --ioengine=posixaio --rw=randwrite --bs=4k --numjobs=1 --size=4g --iodepth=1 runtime=60 --time_based --end_fsync=1
	Test disk with flexible I/O.

 - When a file is deleted on an SSD, a trim operation has to be done to recover that space for a feature use.
 - Most modern Linux file systems automatically run a trim operation every so often, but if you have to do it manually, use the fstrim command.